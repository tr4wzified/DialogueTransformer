using Mutagen.Bethesda;
using Mutagen.Bethesda.Synthesis;
using Mutagen.Bethesda.Skyrim;
using CsvHelper;
using System.Globalization;
using Mutagen.Bethesda.Plugins;
using DialogueTransformer.Common.Models;
using DialogueTransformer.Common;
using System.Diagnostics;
using System.Collections.Concurrent;
using System.Runtime.InteropServices;
using System.Reflection;
using static DialogueTransformer.Common.Enumerations;

namespace DialogueTransformer.Patcher
{
    public class Program
    {
        static Lazy<Settings> Settings = null!;

        public static async Task<int> Main(string[] args)
        {
            return await SynthesisPipeline.Instance
                .AddPatch<ISkyrimMod, ISkyrimModGetter>(RunPatch)
                .SetAutogeneratedSettings(
                    nickname: "Settings",
                    path: "settings.json",
                    out Settings)
                .SetTypicalOpen(GameRelease.SkyrimSE, $"DialogueTransformer.esp")
                .Run(args);
        }

        public static async void RunPatch(IPatcherState<ISkyrimMod, ISkyrimModGetter> state)
        {

            if (!state.InternalDataPath.HasValue)
                throw new Exception("InternalDataPath was null - patcher cannot function!");

            var settings = Settings.Value;
            Console.WriteLine($"<------------------------------------------->");
            Console.WriteLine($"< Running DialogueTransformer by trawzified >");
            Console.WriteLine($"<---------------- Settings ----------------->");
            Console.WriteLine(settings.ToString());
            Console.WriteLine($"<------------------------------------------->");

            var availableModels = Helper.GetModels(state.DataFolderPath);
            var selectedModel = availableModels[settings.Model];
            if (!selectedModel.Installed)
                throw new Exception($"> Selected model {settings.Model}, but it's not installed! Exiting. You can download it here: {selectedModel.DownloadUrl}");

            Dictionary<string, List<IDialogTopicGetter>> dialogueNeedingInferencing = new();

            foreach (var dialogTopic in state.LoadOrder.PriorityOrder.DialogTopic().WinningContextOverrides())
            {
                var name = dialogTopic.Record.Name?.String;
                if (string.IsNullOrWhiteSpace(name))
                    continue;

                // First try to use the previously translated records from analyzed Khajiit patches
                if (settings.UseOverrides && selectedModel.Overrides.TryGetValue(dialogTopic.Record.FormKey, out var dialogTranslation))
                {
                    var translatedDialog = dialogTopic.Record.DeepCopy();
                    translatedDialog.Name = dialogTranslation.TargetText;
                    state.PatchMod.DialogTopics.GetOrAddAsOverride(translatedDialog);
                    continue;
                }

                // Check if this is a sentence or just some unused keyword kinda thing, if none of these characters are in the string just skip it to save useless processing time on the language model
                if (name.StartsWith('(') || name.IndexOf(' ') == -1 || name.IndexOfAny(new char[] { '.', '?', '!' }) == -1)
                    continue;

                if (dialogueNeedingInferencing.TryGetValue(name, out var dialogTopics))
                    dialogTopics.Add(dialogTopic.Record);
                else
                    dialogueNeedingInferencing[name] = new List<IDialogTopicGetter>() { dialogTopic.Record };
            }

            if (!dialogueNeedingInferencing.Any())
            {
                Console.WriteLine("> Patching complete as no dialogue needs inferencing.");
                return;
            }
            Console.WriteLine($"> {dialogueNeedingInferencing.Count} total dialogue records need inferencing");

            var inferencingClientPath = Path.Combine(state.InternalDataPath, Consts.INFERENCING_EXE_FOLDER);

            bool preCacheHasRecords = selectedModel.PreCache.Any();
            bool localCacheHasRecords = selectedModel.LocalCache.Any();
            if (preCacheHasRecords || localCacheHasRecords)
            {
                int preCachedCount = 0, localCachedCount = 0;
                Console.WriteLine($"> Found {selectedModel.PreCache.Count} pre-cached dialogue records");
                Console.WriteLine($"> Found {selectedModel.LocalCache.Count} locally cached dialogue records");
                foreach (var (sourceText, dialogTopicGetters) in dialogueNeedingInferencing)
                {
                    bool foundRecordInCache = false;
                    string? inferencedText = null;
                    if (preCacheHasRecords)
                    {
                        foundRecordInCache = selectedModel.PreCache.TryGetValue(sourceText, out inferencedText);
                        if(foundRecordInCache) preCachedCount++;
                    }
                    if (!foundRecordInCache && localCacheHasRecords)
                    {
                        foundRecordInCache = selectedModel.LocalCache.TryGetValue(sourceText, out inferencedText);
                        if(foundRecordInCache) localCachedCount++;
                    }

                    if (!foundRecordInCache)
                        continue;

                    foreach (var dialogTopicGetter in dialogTopicGetters)
                    {
                        var recordCopy = dialogTopicGetter.DeepCopy();
                        recordCopy.Name!.Set(Mutagen.Bethesda.Strings.Language.English, inferencedText);
                        state.PatchMod.DialogTopics.GetOrAddAsOverride(recordCopy);
                    }
                    dialogueNeedingInferencing.Remove(sourceText);
                }
                Console.WriteLine($"> Resolved {preCachedCount} lines from pre-cache, {localCachedCount} from local cache - {dialogueNeedingInferencing.Count} records yet to be inferenced");
            }

            if (dialogueNeedingInferencing.Any())
            {
                var memoryAmount = Helper.GetTotalMemory();
                // Half of the installed memory in the system divided by 2, in GB
                var maxAllocatedMemory = (((memoryAmount - 2048000000) / 1024000000) / 2);
                // Number of processors
                var processorCount = Environment.ProcessorCount;
                var predictionClientMemoryNeededInGB = 3;
                var threadAmount = (int)(maxAllocatedMemory / (ulong)predictionClientMemoryNeededInGB);
                var chunkedDialogTopics = dialogueNeedingInferencing.Chunk(dialogueNeedingInferencing.Count / threadAmount).Select(chunk => chunk.ToDictionary(x => x.Key, x => x.Value)).ToList();
                Console.WriteLine($"> Inferencing {dialogueNeedingInferencing.Count} dialogue lines using LLM spread over {threadAmount} threads...");
                var sw = Stopwatch.StartNew();
                Task[] tasks = new Task[chunkedDialogTopics.Count];
                int inferencedAmount = 0;
                int printPercentageStep = dialogueNeedingInferencing.Count <= 20 ? 1 : dialogueNeedingInferencing.Count / 20;
                for (int i = 0; i < chunkedDialogTopics.Count; i++)
                {
                    var currentDictionary = chunkedDialogTopics[i];
                    tasks[i] = Task.Run(() =>
                    {
                        var client = new InferencingClient(inferencingClientPath, Path.Combine(selectedModel.Directory.FullName, Consts.MODEL_SUBDIR_NAME), selectedModel.Prefix ?? string.Empty);
                        foreach (var (sourceText, dialogTopics) in currentDictionary)
                        {
                            var inferencedText = client.Inference(sourceText);
                            foreach (var dialogTopic in dialogTopics)
                            {
                                var copiedTopic = dialogTopic.DeepCopy();
                                copiedTopic.Name?.Set(Mutagen.Bethesda.Strings.Language.English, inferencedText);
                                state.PatchMod.DialogTopics.GetOrAddAsOverride(copiedTopic);
                            }
                            selectedModel.LocalCache.TryAdd(sourceText, inferencedText);
                            Interlocked.Increment(ref inferencedAmount);
                            if (inferencedAmount % printPercentageStep == 0)
                                Console.WriteLine($"> Processed {inferencedAmount}/{dialogueNeedingInferencing.Count} records ({Convert.ToInt32(inferencedAmount / dialogueNeedingInferencing.Count * 100)}% done)");
                        }
                    });
                }
                /*
                _ = Task.Run(() =>
                {
                    while (inferencedAmount < dialogueNeedingInferencing.Count)
                    {
                        Thread.Sleep(30000);
                        Console.WriteLine($"Inferencing dialogue... {((int)(inferencedAmount / dialogueNeedingInferencing.Count * 100))}% done");
                    }
                });
                */
                Task.WhenAll(tasks).Wait();
                sw.Stop();
                Console.WriteLine($"> Took {sw.Elapsed.TotalSeconds} sec to inference {dialogueNeedingInferencing.Count} records.");
                Console.WriteLine($"> Saving local cache for {selectedModel.LocalCache.Count} records...");
                Helper.WriteToFile(selectedModel.LocalCache.Select(x => new DialogueTextConversion(x.Key, x.Value)), Path.Combine(selectedModel.Directory.FullName, $"{Consts.LOCAL_CACHE_FILENAME}.{Consts.DATA_FORMAT}"));
                Console.WriteLine($"> Saved!");
            }
        }
    }
}
